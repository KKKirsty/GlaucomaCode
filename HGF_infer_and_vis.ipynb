{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db384ba8",
   "metadata": {},
   "source": [
    "\n",
    "# Harvard-GF — Inference + Visualization (TDS, 52 points)\n",
    "\n",
    "This notebook runs **inference** with your trained backbone (**ViT** or **DINOv3-ConvNeXt**) and renders **52-point TDS** visualizations similar to the paper figure (Input | Ground truth | Prediction).  \n",
    "It also **exports** predictions and targets to Excel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11dd10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.getcwd()) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896aa9b0",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f024e110",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from typing import List, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import nn\n",
    "import timm\n",
    "\n",
    "# Project utilities (must exist in your repo)\n",
    "from dataset import HarvardGFDataset\n",
    "from utils import get_vit_transform, get_cnn_transform, get_albumentations_transform, get_imagenet_transform\n",
    "\n",
    "# HF model for DINOv3\n",
    "from transformers import AutoModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from dino_model import DinoV3Backbone52\n",
    "\n",
    "\n",
    "# ==== Config ====\n",
    "# Paths\n",
    "hgf_dir = '/mnt/sda/sijiali/DataSet/Harvard-GF/Dataset/Test'   # dir with .npz\n",
    "weight_path = '/mnt/sda/sijiali/GlaucomaCode/Results_HGF/dinov3/dinov3_imagenet_lr5e-3/ckpts/best_model_epoch_18_rmse_5.7145.pth'\n",
    "out_dir = './Results_visualization'\n",
    "pred_filename = 'test_pred.xlsx'\n",
    "tar_filename  = 'test_tar.xlsx'\n",
    "\n",
    "# Model\n",
    "backbone = 'dinov3'  # 'vit' or 'dinov3'\n",
    "hf_model_name = '/mnt/sda/sijiali/GlaucomaCode/pretrained_weight/dinov3-vitb16-pretrain-lvd1689m'  # used when backbone='dinov3'\n",
    "vit_pool = 'mean_patch'  # 'cls' or 'mean_patch'; used when backbone='dinov3'\n",
    "device = 'cuda'\n",
    "\n",
    "# Data / transforms\n",
    "modality_type = 'rnflt'\n",
    "task = 'tds'\n",
    "resolution = 224\n",
    "depth = 3\n",
    "transform_name = 'vit'   # 'vit' | 'cnn' | 'albumentations' | 'none' | 'imagenet'\n",
    "\n",
    "# Subset selection\n",
    "mode = 'first_n'  # 'first_n' or 'ids'\n",
    "n = 6             # only used when mode='first_n'\n",
    "ids = ''          # comma-separated; used when mode='ids'\n",
    "ids_file = ''     # a file with one id per line; used when mode='ids'\n",
    "\n",
    "# Inference params\n",
    "batch_size = 1\n",
    "num_workers = 4\n",
    "clamp = False\n",
    "\n",
    "# Scale (IMPORTANT): if you trained with labels/10, keep scale=10.0 so we multiply predictions back.\n",
    "scale = 10.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9162ec23",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714ae08d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Helpers: list/pick ids ====\n",
    "def list_all_ids(hgf_dir: str) -> List[str]:\n",
    "    ids = sorted([Path(f).stem for f in os.listdir(hgf_dir) if f.endswith('.npz')])\n",
    "    if not ids:\n",
    "        raise RuntimeError(f'No .npz files found in {hgf_dir}')\n",
    "    return ids\n",
    "\n",
    "def pick_ids(hgf_dir: str, mode: str, n: int = 0, ids: Optional[List[str]] = None) -> List[str]:\n",
    "    all_ids = list_all_ids(hgf_dir)\n",
    "    if mode == 'first_n':\n",
    "        assert n > 0, 'n must be > 0 for mode=first_n'\n",
    "        return all_ids[:n]\n",
    "    elif mode == 'ids':\n",
    "        assert ids and len(ids) > 0, 'mode=ids requires ids'\n",
    "        aset = set(all_ids)\n",
    "        return [i for i in ids if i in aset]\n",
    "    else:\n",
    "        raise ValueError(f'Unknown mode: {mode}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c900685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Build target DataFrame from .npz ====\n",
    "def build_tar_df_from_npz(hgf_dir: str, picked_ids: List[str], lat_fill: str = 'Unknown') -> pd.DataFrame:\n",
    "    vecs = []\n",
    "    for sid in picked_ids:\n",
    "        npz_path = os.path.join(hgf_dir, f'{sid}.npz')\n",
    "        raw = np.load(npz_path, allow_pickle=True)\n",
    "        if 'tds' not in raw.files:\n",
    "            raise KeyError(f\"'tds' not in {npz_path}, keys={raw.files}\")\n",
    "        vecs.append(raw['tds'].astype(np.float32))\n",
    "    mat = np.stack(vecs, axis=0)  # (N,K)\n",
    "    cols = [f'tar_{i}' for i in range(mat.shape[1])]\n",
    "    df = pd.DataFrame(mat, columns=cols)\n",
    "    df.insert(0, 'test_id', picked_ids)\n",
    "    df.insert(1, 'test_lat', [lat_fill] * len(picked_ids))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b65c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Sequential sampler on specific indices ====\n",
    "class SeqOnIndices(torch.utils.data.Sampler):\n",
    "    def __init__(self, indices: List[int]):\n",
    "        self.indices = indices\n",
    "    def __iter__(self):\n",
    "        return iter(self.indices)\n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "\n",
    "# ==== DataLoader subset ====\n",
    "def get_hgf_loader_subset(\n",
    "    hgf_dir: str,\n",
    "    modality_type: str,\n",
    "    task: str,\n",
    "    resolution: int,\n",
    "    depth: int,\n",
    "    transform,\n",
    "    picked_ids: List[str],\n",
    "    batch_size: int = 1,\n",
    "    num_workers: int = 4,\n",
    ") -> DataLoader:\n",
    "    ds = HarvardGFDataset(\n",
    "        data_path=hgf_dir,\n",
    "        modality_type=modality_type,\n",
    "        task=task,\n",
    "        resolution=resolution,\n",
    "        depth=depth,\n",
    "        transform=transform,\n",
    "    )\n",
    "    id2idx = {Path(f).stem: i for i, f in enumerate(ds.files)}\n",
    "    indices = [id2idx[s] for s in picked_ids if s in id2idx]\n",
    "    sampler = SeqOnIndices(indices)\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        sampler=sampler,\n",
    "        pin_memory=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2086963",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== 52-point layout & visualization ====\n",
    "HARVARD_GF_52_LAYOUT = [\n",
    "    [None,  0,  1,  2,  3,  4,  5, None],\n",
    "    [  6,   7,  8,  9, 10, 11, 12, 13],\n",
    "    [ 14,  15, 16, 17, 18, 19, 20, 21],\n",
    "    [ 22,  23, 24, 25, 26, 27, 28, 29],\n",
    "    [ 30,  31, 32, 33, 34, 35, 36, 37],\n",
    "    [ 38,  39, 40, 41, 42, 43, 44, 45],\n",
    "    [None, 46, 47, 48, 49, 50, 51, None],\n",
    "]\n",
    "\n",
    "def _draw_vf(ax, values, layout=HARVARD_GF_52_LAYOUT, vmin=None, vmax=None,\n",
    "             cmap='gray', fontsize=8, fmt='{:.1f}'):\n",
    "    values = np.asarray(values).reshape(-1)\n",
    "    assert values.size == 52, f'Expect 52 values, got {values.size}'\n",
    "    if vmin is None: vmin = float(np.nanmin(values))\n",
    "    if vmax is None: vmax = float(np.nanmax(values))\n",
    "    norm = Normalize(vmin=vmin, vmax=vmax)\n",
    "    ax.set_aspect('equal'); ax.axis('off')\n",
    "    n_rows, n_cols = len(layout), len(layout[0])\n",
    "    for r in range(n_rows):\n",
    "        for c in range(n_cols):\n",
    "            idx = layout[r][c]\n",
    "            if idx is None: continue\n",
    "            v = float(values[idx])\n",
    "            rect = Rectangle((c, r), 1, 1, facecolor=plt.get_cmap(cmap)(norm(v)),\n",
    "                             edgecolor='k', linewidth=0.5)\n",
    "            ax.add_patch(rect)\n",
    "            lum = norm(v)\n",
    "            txt_color = 'white' if lum < 0.4 else 'black'\n",
    "            ax.text(c+0.5, r+0.5, fmt.format(v), ha='center', va='center',\n",
    "                    fontsize=fontsize, color=txt_color)\n",
    "    ax.set_xlim(0, n_cols); ax.set_ylim(n_rows, 0)\n",
    "\n",
    "def plot_vf_triptych(input_img, gt_52, pred_52, layout=HARVARD_GF_52_LAYOUT,\n",
    "                     vmin=None, vmax=None, save_path=None, titles=('Input','Ground truth','Prediction')):\n",
    "    gt_52  = np.asarray(gt_52).reshape(-1)\n",
    "    pred_52 = np.asarray(pred_52).reshape(-1)\n",
    "    if vmin is None: vmin = float(np.nanmin([gt_52.min(), pred_52.min()]))\n",
    "    if vmax is None: vmax = float(np.nanmax([gt_52.max(), pred_52.max()]))\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(10, 4), gridspec_kw={'width_ratios':[1.2,1,1]})\n",
    "    # Input\n",
    "    ax0 = axes[0]; ax0.axis('off'); ax0.set_title(titles[0])\n",
    "\n",
    "    img = input_img\n",
    "    # 1) 转 numpy\n",
    "    if torch.is_tensor(img):\n",
    "        img = img.detach().cpu()\n",
    "        # 如果是 (C,H,W) 且 C=1/3，先转 (H,W,C)\n",
    "        if img.ndim == 3 and img.shape[0] in (1,3):\n",
    "            img = img.permute(1, 2, 0)\n",
    "        img = img.numpy()\n",
    "\n",
    "    # 2) 处理通道数\n",
    "    if img.ndim == 2:\n",
    "        # (H,W) 灰度 -> 扩成 3 通道\n",
    "        img = np.repeat(img[..., None], 3, axis=2)\n",
    "    elif img.ndim == 3 and img.shape[2] == 1:\n",
    "        # (H,W,1) -> (H,W,3)\n",
    "        img = np.repeat(img, 3, axis=2)\n",
    "\n",
    "    # 3) 数值范围（如果是 float 且不是 uint8，clip 到 [0,1]）\n",
    "    if img.dtype != np.uint8:\n",
    "        img = np.clip(img, 0.0, 1.0)\n",
    "\n",
    "    ax0.imshow(img)\n",
    "    # GT & Pred\n",
    "    axes[1].set_title(titles[1]); _draw_vf(axes[1], gt_52, layout, vmin, vmax)\n",
    "    axes[2].set_title(titles[2]); _draw_vf(axes[2], pred_52, layout, vmin, vmax)\n",
    "    plt.tight_layout()\n",
    "    if save_path: plt.savefig(save_path, dpi=200); plt.close(fig)\n",
    "    else: plt.show()\n",
    "\n",
    "def preview_layout(layout=HARVARD_GF_52_LAYOUT):\n",
    "    dummy = np.arange(52)\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    _draw_vf(ax, dummy, layout, vmin=0, vmax=51, fmt='{:.0f}')\n",
    "    ax.set_title('Index layout (0..51)'); plt.tight_layout(); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713a534d",
   "metadata": {},
   "source": [
    "## Infer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a487b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Build model, DataLoader, run inference, export ====\n",
    "# 1) Pick IDs\n",
    "if mode == 'first_n':\n",
    "    picked_ids = pick_ids(hgf_dir, 'first_n', n=n)\n",
    "else:\n",
    "    id_list = []\n",
    "    if ids:      id_list += [s.strip() for s in ids.split(',') if s.strip()]\n",
    "    if ids_file:\n",
    "        with open(ids_file, 'r') as f:\n",
    "            id_list += [line.strip() for line in f if line.strip()]\n",
    "    picked_ids = pick_ids(hgf_dir, 'ids', ids=id_list)\n",
    "print(f'[HGF] picked {len(picked_ids)} samples.')\n",
    "\n",
    "# 2) Transform\n",
    "mean = np.array([0.485, 0.456, 0.406]); std = np.array([0.229, 0.224, 0.225])\n",
    "if transform_name == 'vit':\n",
    "    transform = get_vit_transform(resolution)\n",
    "elif transform_name == 'cnn':\n",
    "    transform = get_cnn_transform(resolution, mean, std)\n",
    "elif transform_name == 'albumentations':\n",
    "    transform = get_albumentations_transform(resolution)\n",
    "elif transform_name == 'none':\n",
    "    transform = None\n",
    "elif transform_name == 'imagenet':\n",
    "    transform = get_imagenet_transform(resolution)\n",
    "else:\n",
    "    raise ValueError(f'Unknown transform: {transform_name}')\n",
    "\n",
    "# 3) DataLoader\n",
    "test_loader = get_hgf_loader_subset(\n",
    "    hgf_dir=hgf_dir,\n",
    "    modality_type=modality_type,\n",
    "    task=task,\n",
    "    resolution=resolution,\n",
    "    depth=depth,\n",
    "    transform=transform,\n",
    "    picked_ids=picked_ids,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "all_test_id  = picked_ids\n",
    "all_test_lat = ['Unknown'] * len(picked_ids)\n",
    "\n",
    "# 4) Model\n",
    "num_classes = 52\n",
    "if backbone == 'vit':\n",
    "    model = timm.create_model(\n",
    "        'vit_base_patch8_224.augreg2_in21k_ft_in1k',\n",
    "        pretrained=False,\n",
    "        pretrained_cfg_overlay=dict(file='/mnt/sda/sijiali/GlaucomaCode/vit_base_patch8_224.augreg2_in1k_ft_in1k/pytorch_model.bin')\n",
    "    )\n",
    "    dim = model.head.in_features\n",
    "    model.head = nn.Sequential(nn.ReLU(), nn.Linear(dim, num_classes))\n",
    "elif backbone == 'dinov3':\n",
    "    model = DinoV3Backbone52(hf_model_name=hf_model_name, out_dim=num_classes,\n",
    "                             apply_imagenet_norm=False, vit_pool=vit_pool)\n",
    "else:\n",
    "    raise ValueError(f'Unknown backbone: {backbone}')\n",
    "\n",
    "# 5) Load weights (handle DataParallel prefix)\n",
    "def _strip_module_prefix(state_dict):\n",
    "    if any(k.startswith('module.') for k in state_dict.keys()):\n",
    "        return {k.replace('module.', '', 1): v for k, v in state_dict.items()}\n",
    "    return state_dict\n",
    "\n",
    "ckpt = torch.load(weight_path, map_location='cpu')\n",
    "state = ckpt.get('model_state', ckpt)\n",
    "state = _strip_module_prefix(state)\n",
    "missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "print('[load] missing:', missing, '| unexpected:', unexpected)\n",
    "\n",
    "# 6) Inference loop\n",
    "model.eval(); model.to(device)\n",
    "pred_rows = []\n",
    "imgs_cache = []   # keep a few images for visualization\n",
    "with torch.no_grad():\n",
    "    for i, batch in enumerate(test_loader):\n",
    "        x = batch['image'].to(device)\n",
    "        y = batch.get('label', None)\n",
    "        out_scaled = model(x)                    # [B,52] in scaled space\n",
    "        out = out_scaled * scale                 # back to original unit\n",
    "        out_np = out.detach().cpu().numpy()\n",
    "        # keep for export\n",
    "        sid = all_test_id[i]\n",
    "        one = {'test_id': sid, 'test_lat': all_test_lat[i]}\n",
    "        for k in range(52):\n",
    "            one[f'pred_{k}'] = float(out_np[0, k])\n",
    "        pred_rows.append(one)\n",
    "        # cache a few images + gt for visualization\n",
    "        if i < 12:\n",
    "            img = batch['image'][0]  # (C,H,W)\n",
    "            gt  = (batch['label'][0].numpy() if y is not None else np.full(52, np.nan))\n",
    "            imgs_cache.append((sid, img, gt, out_np[0]))\n",
    "\n",
    "pred_df = pd.DataFrame(pred_rows)\n",
    "print(pred_df.head())\n",
    "\n",
    "# 7) Export predictions & targets\n",
    "Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
    "pred_xlsx = os.path.join(out_dir, pred_filename)\n",
    "pred_df.to_excel(pred_xlsx, index=False)\n",
    "print(f'[SAVE] predictions -> {pred_xlsx}')\n",
    "\n",
    "tar_df = build_tar_df_from_npz(hgf_dir, all_test_id, lat_fill='Unknown')\n",
    "tar_xlsx = os.path.join(out_dir, tar_filename)\n",
    "tar_df.to_excel(tar_xlsx, index=False)\n",
    "print(f'[SAVE] targets -> {tar_xlsx}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25877c63",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f65464",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Visualization: layout preview ====\n",
    "preview_layout(HARVARD_GF_52_LAYOUT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997dd435",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==== Visualization: a few examples ====\n",
    "# vmin/vmax you can fix, e.g., vmin=-30, vmax=5 for TDS-like ranges.\n",
    "vmin, vmax = None, None  # or set to (-30, 5)\n",
    "\n",
    "save_pngs = True\n",
    "save_dir = os.path.join(out_dir, 'figs')\n",
    "Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for sid, img, gt, pred_scaled in imgs_cache:\n",
    "    pred = pred_scaled * 1.0   # already scaled back earlier (out = out_scaled*scale)\n",
    "    save_path = os.path.join(save_dir, f'{sid}.png') if save_pngs else None\n",
    "    plot_vf_triptych(\n",
    "        input_img=img, \n",
    "        gt_52=gt, \n",
    "        pred_52=pred, \n",
    "        vmin=vmin, vmax=vmax, \n",
    "        save_path=save_path\n",
    "    )\n",
    "    if not save_pngs:\n",
    "        break  # if not saving, display only the first one\n",
    "print(f'[SAVE] figures -> {save_dir}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glaucoma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
