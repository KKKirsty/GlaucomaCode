{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0d4160a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, math, random, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# >>> 项目根目录\n",
    "PROJECT_ROOT = \"/mnt/sda/sijiali/GlaucomaCode\"\n",
    "import sys\n",
    "if PROJECT_ROOT not in sys.path:\n",
    "    sys.path.append(PROJECT_ROOT)\n",
    "\n",
    "\n",
    "from utils import get_imagenet_transform, _plot_quad\n",
    "from train import get_dataloader_Salsa, SalsaHGFAlignedDataset\n",
    "from dino_model import DualDinoV3LateFusion52  \n",
    "from train import _make_split_ids\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8e4791b",
   "metadata": {},
   "source": [
    "# 1. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ad884f32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# model\n",
    "backbone      = \"dinov3\"                                 # 仅支持 dinov3\n",
    "dinov3_model  = \"/mnt/sda/sijiali/GlaucomaCode/pretrained_weight/dinov3-vitb16-pretrain-lvd1689m\"\n",
    "fusion        = \"attn\"                                  # \"concat\" / \"gated-sum\" / \"sum\" / \"attn\"\n",
    "vit_pool      = \"cls\"                                     # \"cls\" 或 \"mean_patch\"\n",
    "train_scope   = \"all\"                                    # \"head\" 或 \"all\"\n",
    "\n",
    "if fusion == \"concat\":\n",
    "    ckpt_path = \"/mnt/sda/sijiali/GlaucomaCode/Results_SALSA_multi/dinov3/dinov3_imagenet_all_cls_concat_lr5e-5/ckpts/best_model_epoch_302_rmse_1.0466.pth\" \n",
    "elif fusion == \"gated-sum\":\n",
    "    ckpt_path = \"/mnt/sda/sijiali/GlaucomaCode/Results_SALSA_multi/dinov3/dinov3_imagenet_all_cls_gated-sum_lr5e-5/ckpts/best_model_epoch_379_rmse_1.0206.pth\"\n",
    "elif fusion == \"sum\":\n",
    "    ckpt_path = \"/mnt/sda/sijiali/GlaucomaCode/Results_SALSA_multi/dinov3/dinov3_imagenet_all_cls_sum_lr1e-4/ckpts/best_model_epoch_288_rmse_0.9195.pth\"\n",
    "elif fusion == \"attn\":\n",
    "    ckpt_path = \"/mnt/sda/sijiali/GlaucomaCode/Results_SALSA_multi/dinov3/dinov3_imagenet_all_cls_attn_lr1e-4/ckpts/best_model_epoch_422_rmse_1.0707.pth\"\n",
    "else:\n",
    "    raise ValueError(f\"未知 fusion: {fusion}\")\n",
    "# \"/mnt/sda/sijiali/GlaucomaCode/Results_SALSA_multi/dinov3/dinov3_imagenet_all_cls_attn_lr1e-4/ckpts/best_model_epoch_422_rmse_1.0707.pth\"\n",
    "# \"/mnt/sda/sijiali/GlaucomaCode/Results_SALSA_multi/dinov3/dinov3_imagenet_all_cls_sum_lr1e-4/ckpts/best_model_epoch_288_rmse_0.9195.pth\"\n",
    "# \"/mnt/sda/sijiali/GlaucomaCode/Results_SALSA_multi/dinov3/dinov3_imagenet_all_cls_gated-sum_lr5e-5/ckpts/best_model_epoch_379_rmse_1.0206.pth\"\n",
    "# \"/mnt/sda/sijiali/GlaucomaCode/Results_SALSA_multi/dinov3/dinov3_imagenet_all_cls_concat_lr5e-5/ckpts/best_model_epoch_302_rmse_1.0466.pth\"   # 可视化的最优权重\n",
    "\n",
    "# data\n",
    "transform     = \"imagenet\"                                # \"vit\"/\"cnn\"/\"albumentations\"/\"none\"/\"imagenet\"\n",
    "modality_type = \"rnflt+slab\"                             # \"rnflt\" 或 \"rnflt+slab\"\n",
    "img_size      = 224                                       # 输入尺寸\n",
    "data_root     = \"/mnt/sda/sijiali/DataSet/harvardGF_unpacked\"\n",
    "hgf_test_root = \"/mnt/sda/sijiali/DataSet/Harvard-GF/Dataset/Test\"\n",
    "val_ratio    = 0.1            # 仅当没有 split_txt 时用于临时划分\n",
    "split_txt    = None           # 若你有固定验证集ID文件，填路径；否则保持 None\n",
    "VFSCALE = 10\n",
    "\n",
    "# 选样策略\n",
    "n_random     = 6            # 从验证集随机挑 n 个样本\n",
    "specified_ids= []             # 或者给定 [\"data_2401\", \"data_1234\"]，留空表示不用\n",
    "\n",
    "# Grad-CAM 设置\n",
    "target_index = None   # 例如 10 表示第 11 个 VF 点；None 表示对 52 维取和\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "819677bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_basedir = Path(\"./Results_visualization/gradcam_out\") \n",
    "out_dir = out_basedir / (ckpt_path.split(\"/\")[-3] + \"_\" + ckpt_path.split(\"/\")[-1].replace(\".pth\",\"\"))\n",
    "out_dir.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bdffa3",
   "metadata": {},
   "source": [
    "# 2. Build validation set & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1fbdf665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "split => train=627, val=69\n",
      "Chosen IDs: ['data_2553', 'data_3182', 'data_2658', 'data_2403', 'data_3052', 'data_3068']\n"
     ]
    }
   ],
   "source": [
    "train_ids, val_ids = _make_split_ids(data_root, val_ratio=val_ratio, split_txt=split_txt, seed=42)\n",
    "print(f\"split => train={len(train_ids)}, val={len(val_ids)}\")\n",
    "\n",
    "# 选取要可视化的样本 ID 列表\n",
    "if specified_ids:\n",
    "    chosen_ids = [sid for sid in specified_ids if sid in val_ids]\n",
    "else:\n",
    "    rng = random.Random(2025)\n",
    "    chosen_ids = rng.sample(val_ids, k=min(n_random, len(val_ids)))\n",
    "\n",
    "print(\"Chosen IDs:\", chosen_ids)\n",
    "\n",
    "# transform：与训练一致\n",
    "base_transform = get_imagenet_transform(img_size, with_slab=(modality_type=='rnflt+slab'))\n",
    "\n",
    "# val 子集（通过 ids 精确采样）\n",
    "val_loader = get_dataloader_Salsa(\n",
    "    image_root=data_root,\n",
    "    hgf_test_root=hgf_test_root,\n",
    "    modality_type=modality_type,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    transform=base_transform,\n",
    "    ids=chosen_ids\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9920e3f1",
   "metadata": {},
   "source": [
    "# 3. Build model & load trained weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d672e856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ckpt] loaded. missing: [] unexpected: []\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DualDinoV3LateFusion52(\n",
       "  (backbone_r): DinoV3FeatureBackbone(\n",
       "    (net): DINOv3ViTModel(\n",
       "      (embeddings): DINOv3ViTEmbeddings(\n",
       "        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (rope_embeddings): DINOv3ViTRopePositionEmbedding()\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DINOv3ViTLayer(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): DINOv3ViTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_scale1): DINOv3ViTLayerScale()\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): DINOv3ViTMLP(\n",
       "            (up_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (down_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act_fn): GELUActivation()\n",
       "          )\n",
       "          (layer_scale2): DINOv3ViTLayerScale()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (backbone_s): DinoV3FeatureBackbone(\n",
       "    (net): DINOv3ViTModel(\n",
       "      (embeddings): DINOv3ViTEmbeddings(\n",
       "        (patch_embeddings): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (rope_embeddings): DINOv3ViTRopePositionEmbedding()\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x DINOv3ViTLayer(\n",
       "          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (attention): DINOv3ViTAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (o_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (layer_scale1): DINOv3ViTLayerScale()\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (mlp): DINOv3ViTMLP(\n",
       "            (up_proj): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (down_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (act_fn): GELUActivation()\n",
       "          )\n",
       "          (layer_scale2): DINOv3ViTLayerScale()\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (head): LateFusionHead52(\n",
       "    (proj_r): Identity()\n",
       "    (proj_s): Identity()\n",
       "    (q): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (k): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (v): Linear(in_features=768, out_features=768, bias=False)\n",
       "    (attn_ln): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (merge): Sequential(\n",
       "      (0): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (1): Linear(in_features=768, out_features=512, bias=True)\n",
       "      (2): GELU(approximate='none')\n",
       "      (3): Dropout(p=0.1, inplace=False)\n",
       "      (4): Linear(in_features=512, out_features=52, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DualDinoV3LateFusion52(\n",
    "    rnflt_model_name=dinov3_model,\n",
    "    rnflt_vit_pool=vit_pool,\n",
    "    slab_model_name=dinov3_model,\n",
    "    slab_vit_pool=vit_pool,\n",
    "    fusion=fusion,\n",
    "    head_hidden_dim=512,\n",
    "    head_dropout=0.1,\n",
    "    out_dim=52\n",
    ").to(device)\n",
    "\n",
    "# 加载训练权重\n",
    "def load_ckpt_strict_flexible(model, ckpt_path):\n",
    "    if ckpt_path is None or not os.path.exists(ckpt_path):\n",
    "        print(\"No ckpt loaded.\")\n",
    "        return\n",
    "    state = torch.load(ckpt_path, map_location=device)\n",
    "    if isinstance(state, dict) and \"state_dict\" in state:\n",
    "        state = state[\"state_dict\"]\n",
    "    # 去掉可能的 \"module.\" 前缀\n",
    "    new_state = {}\n",
    "    for k, v in state.items():\n",
    "        new_state[k.replace(\"module.\", \"\")] = v\n",
    "    missing, unexpected = model.load_state_dict(new_state, strict=False)\n",
    "    print(\"[ckpt] loaded. missing:\", missing, \"unexpected:\", unexpected)\n",
    "\n",
    "load_ckpt_strict_flexible(model, ckpt_path)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c55d3c2d",
   "metadata": {},
   "source": [
    "# 4. Grad-CAM helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b1474177",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, LayerCAM, GradCAMPlusPlus\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "# 目标函数：index=None 表示所有 52 维的均值；否则指定某个 VF 点索引\n",
    "class VFPointTarget(ClassifierOutputTarget):\n",
    "    def __init__(self, index=None):\n",
    "        self.index = index\n",
    "    def __call__(self, model_output):\n",
    "        if self.index is None:\n",
    "            return model_output.sum()\n",
    "        return model_output[:, int(self.index)]\n",
    "\n",
    "# ViT 的 token -> [C,H,W] reshape（去掉 CLS 与 register tokens）\n",
    "def make_vit_reshape_transform(num_register_tokens=0):\n",
    "    def _rt(x):\n",
    "        if isinstance(x, (list, tuple)): x = x[0]\n",
    "        B, T, C = x.shape\n",
    "        start = 1 + int(num_register_tokens)  # 跳过 CLS(+reg)\n",
    "        N = T - start\n",
    "        S = int(round(N ** 0.5))             # 期待 14\n",
    "        assert S*S == N, f\"patch tokens={N} 不是正方形\"\n",
    "        return x[:, start:, :].reshape(B, S, S, C).permute(0,3,1,2).contiguous()\n",
    "    return _rt\n",
    "\n",
    "# 自动定位 HF ViT 中“最后一个可 hook 的层”（一般用最后 block 的 LayerNorm）\n",
    "def find_vit_last_layernorm(vit_model):\n",
    "    \"\"\"\n",
    "    尝试在 HF DINOv3 ViT 里定位“最后一个可 hook 的 LN/Norm 层”\n",
    "    按以下优先级：\n",
    "      1) .vision_model.encoder.layers[-1].layernorm_before / .layer_norm1\n",
    "      2) .vit.encoder.layers[-1].layernorm_before / .layer_norm1\n",
    "      3) .encoder.layers[-1].layernorm_before / .layer_norm1\n",
    "      4) .blocks[-1].norm1\n",
    "      5) 常见别名 'ln'/'norm'/'layernorm'/'ln1'/'norm1'/'pre_norm'\n",
    "    \"\"\"\n",
    "    cand_roots = []\n",
    "    for name in [\"vision_model\", \"vit\", \"encoder\", \"\"]:\n",
    "        node = getattr(vit_model, name, None) if name else vit_model\n",
    "        if node is not None:\n",
    "            cand_roots.append(node)\n",
    "\n",
    "    for root in cand_roots:\n",
    "        # encoder.layers[-1]\n",
    "        enc = getattr(root, \"encoder\", None)\n",
    "        layers = getattr(enc, \"layers\", None) if enc is not None else None\n",
    "        if isinstance(layers, (list, tuple)) and len(layers) > 0:\n",
    "            last = layers[-1]\n",
    "            for nm in [\"layernorm_before\", \"layer_norm1\", \"ln1\", \"norm1\", \"pre_norm\"]:\n",
    "                if hasattr(last, nm):\n",
    "                    return getattr(last, nm)\n",
    "\n",
    "    # 退而求其次：blocks[-1].norm1\n",
    "    if hasattr(vit_model, \"blocks\") and isinstance(vit_model.blocks, (list, tuple)) and len(vit_model.blocks) > 0:\n",
    "        last = vit_model.blocks[-1]\n",
    "        for nm in [\"norm1\", \"ln1\", \"layernorm_before\", \"pre_norm\"]:\n",
    "            if hasattr(last, nm):\n",
    "                return getattr(last, nm)\n",
    "\n",
    "    # 最后再试顶层别名\n",
    "    for nm in [\"ln\", \"norm\", \"layernorm\", \"ln1\", \"norm1\", \"pre_norm\"]:\n",
    "        if hasattr(vit_model, nm):\n",
    "            return getattr(vit_model, nm)\n",
    "\n",
    "    raise RuntimeError(\"Cannot locate a ViT LayerNorm to hook for Grad-CAM.\")\n",
    "\n",
    "# numpy 可视化底图（把 [1,3,H,W] 归一化到 0-1 RGB）\n",
    "def to_numpy_rgb01(t):\n",
    "    if t.ndim == 4:\n",
    "        t = t[0]\n",
    "    arr = t.detach().float().cpu().permute(1,2,0).numpy()\n",
    "    arr = (arr - arr.min()) / max(arr.max() - arr.min(), 1e-6)\n",
    "    return arr\n",
    "\n",
    "# 包装器：把另一模态固定，Grad-CAM 按单输入驱动\n",
    "class DualWrapperRNFLT(nn.Module):\n",
    "    def __init__(self, core, fixed_slab):\n",
    "        super().__init__()\n",
    "        self.core = core\n",
    "        self.fixed_slab = fixed_slab\n",
    "    def forward(self, x):\n",
    "        return self.core(x, self.fixed_slab)\n",
    "\n",
    "class DualWrapperSLAB(nn.Module):\n",
    "    def __init__(self, core, fixed_rnflt):\n",
    "        super().__init__()\n",
    "        self.core = core\n",
    "        self.fixed_rnflt = fixed_rnflt\n",
    "    def forward(self, x):\n",
    "        return self.core(self.fixed_rnflt, x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f28701a",
   "metadata": {},
   "source": [
    "# 5. 跑 Grad-CAM（随机 n 个或指定 ID 集合）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "48bc7d06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function BaseCAM.__del__ at 0x7fbf8149eee0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/sda/sijiali/anaconda3/envs/glaucoma/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py\", line 212, in __del__\n",
      "    self.activations_and_grads.release()\n",
      "AttributeError: 'GradCAM' object has no attribute 'activations_and_grads'\n",
      "Exception ignored in: <function BaseCAM.__del__ at 0x7fbf8149eee0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/sda/sijiali/anaconda3/envs/glaucoma/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py\", line 212, in __del__\n",
      "    self.activations_and_grads.release()\n",
      "AttributeError: 'GradCAM' object has no attribute 'activations_and_grads'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] Results_visualization/gradcam_out/dinov3_imagenet_all_cls_attn_lr1e-4_best_model_epoch_422_rmse_1.0707/data_2401_combo_idxsum.png\n",
      "[saved quad] Results_visualization/gradcam_out/dinov3_imagenet_all_cls_attn_lr1e-4_best_model_epoch_422_rmse_1.0707/data_2401_quad_with_metrics.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function BaseCAM.__del__ at 0x7fbf8149eee0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/sda/sijiali/anaconda3/envs/glaucoma/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py\", line 212, in __del__\n",
      "    self.activations_and_grads.release()\n",
      "AttributeError: 'GradCAM' object has no attribute 'activations_and_grads'\n",
      "Exception ignored in: <function BaseCAM.__del__ at 0x7fbf8149eee0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/sda/sijiali/anaconda3/envs/glaucoma/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py\", line 212, in __del__\n",
      "    self.activations_and_grads.release()\n",
      "AttributeError: 'GradCAM' object has no attribute 'activations_and_grads'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] Results_visualization/gradcam_out/dinov3_imagenet_all_cls_attn_lr1e-4_best_model_epoch_422_rmse_1.0707/data_2402_combo_idxsum.png\n",
      "[saved quad] Results_visualization/gradcam_out/dinov3_imagenet_all_cls_attn_lr1e-4_best_model_epoch_422_rmse_1.0707/data_2402_quad_with_metrics.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function BaseCAM.__del__ at 0x7fbf8149eee0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/sda/sijiali/anaconda3/envs/glaucoma/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py\", line 212, in __del__\n",
      "    self.activations_and_grads.release()\n",
      "AttributeError: 'GradCAM' object has no attribute 'activations_and_grads'\n",
      "Exception ignored in: <function BaseCAM.__del__ at 0x7fbf8149eee0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/sda/sijiali/anaconda3/envs/glaucoma/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py\", line 212, in __del__\n",
      "    self.activations_and_grads.release()\n",
      "AttributeError: 'GradCAM' object has no attribute 'activations_and_grads'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] Results_visualization/gradcam_out/dinov3_imagenet_all_cls_attn_lr1e-4_best_model_epoch_422_rmse_1.0707/data_2403_combo_idxsum.png\n",
      "[saved quad] Results_visualization/gradcam_out/dinov3_imagenet_all_cls_attn_lr1e-4_best_model_epoch_422_rmse_1.0707/data_2403_quad_with_metrics.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function BaseCAM.__del__ at 0x7fbf8149eee0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/sda/sijiali/anaconda3/envs/glaucoma/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py\", line 212, in __del__\n",
      "    self.activations_and_grads.release()\n",
      "AttributeError: 'GradCAM' object has no attribute 'activations_and_grads'\n",
      "Exception ignored in: <function BaseCAM.__del__ at 0x7fbf8149eee0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/sda/sijiali/anaconda3/envs/glaucoma/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py\", line 212, in __del__\n",
      "    self.activations_and_grads.release()\n",
      "AttributeError: 'GradCAM' object has no attribute 'activations_and_grads'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] Results_visualization/gradcam_out/dinov3_imagenet_all_cls_attn_lr1e-4_best_model_epoch_422_rmse_1.0707/data_2404_combo_idxsum.png\n",
      "[saved quad] Results_visualization/gradcam_out/dinov3_imagenet_all_cls_attn_lr1e-4_best_model_epoch_422_rmse_1.0707/data_2404_quad_with_metrics.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function BaseCAM.__del__ at 0x7fbf8149eee0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/sda/sijiali/anaconda3/envs/glaucoma/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py\", line 212, in __del__\n",
      "    self.activations_and_grads.release()\n",
      "AttributeError: 'GradCAM' object has no attribute 'activations_and_grads'\n",
      "Exception ignored in: <function BaseCAM.__del__ at 0x7fbf8149eee0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/sda/sijiali/anaconda3/envs/glaucoma/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py\", line 212, in __del__\n",
      "    self.activations_and_grads.release()\n",
      "AttributeError: 'GradCAM' object has no attribute 'activations_and_grads'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] Results_visualization/gradcam_out/dinov3_imagenet_all_cls_attn_lr1e-4_best_model_epoch_422_rmse_1.0707/data_2405_combo_idxsum.png\n",
      "[saved quad] Results_visualization/gradcam_out/dinov3_imagenet_all_cls_attn_lr1e-4_best_model_epoch_422_rmse_1.0707/data_2405_quad_with_metrics.png\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function BaseCAM.__del__ at 0x7fbf8149eee0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/sda/sijiali/anaconda3/envs/glaucoma/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py\", line 212, in __del__\n",
      "    self.activations_and_grads.release()\n",
      "AttributeError: 'GradCAM' object has no attribute 'activations_and_grads'\n",
      "Exception ignored in: <function BaseCAM.__del__ at 0x7fbf8149eee0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/sda/sijiali/anaconda3/envs/glaucoma/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py\", line 212, in __del__\n",
      "    self.activations_and_grads.release()\n",
      "AttributeError: 'GradCAM' object has no attribute 'activations_and_grads'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[saved] Results_visualization/gradcam_out/dinov3_imagenet_all_cls_attn_lr1e-4_best_model_epoch_422_rmse_1.0707/data_2406_combo_idxsum.png\n",
      "[saved quad] Results_visualization/gradcam_out/dinov3_imagenet_all_cls_attn_lr1e-4_best_model_epoch_422_rmse_1.0707/data_2406_quad_with_metrics.png\n",
      "Saved to: /mnt/sda/sijiali/GlaucomaCode/Results_visualization/gradcam_out/dinov3_imagenet_all_cls_attn_lr1e-4_best_model_epoch_422_rmse_1.0707\n",
      "Done IDs: ['data_2401', 'data_2402', 'data_2403', 'data_2404', 'data_2405', 'data_2406']\n"
     ]
    }
   ],
   "source": [
    "target = VFPointTarget(index=target_index)\n",
    "\n",
    "# 选择 ViT 的目标层（各自分支）\n",
    "target_layer_r = model.backbone_r.net.layer[-1].norm1\n",
    "target_layer_s = model.backbone_s.net.layer[-1].norm1\n",
    "# 注意力层（有时更亮）\n",
    "# target_layer_r = model.backbone_r.net.layer[-1].attention\n",
    "# target_layer_s = model.backbone_s.net.layer[-1].attention\n",
    "\n",
    "# ViT reshape（去掉 CLS + register）\n",
    "num_reg = int(getattr(model.backbone_r, \"num_register_tokens\", 0))\n",
    "reshape_fn = make_vit_reshape_transform(num_register_tokens=num_reg)\n",
    "\n",
    "def _make_cam(model_wrapper, target_layers, reshape_transform, use_cuda):\n",
    "    \"\"\"返回一个可作为 with 使用的 GradCAM 对象，兼容不同版本\"\"\"\n",
    "    try:\n",
    "        return GradCAM(model=model_wrapper,\n",
    "                       target_layers=target_layers,\n",
    "                       reshape_transform=reshape_transform,\n",
    "                       use_cuda=use_cuda)\n",
    "    except TypeError:\n",
    "        return GradCAM(model=model_wrapper,\n",
    "                       target_layers=target_layers,\n",
    "                       reshape_transform=reshape_transform)\n",
    "\n",
    "use_cuda = (getattr(device, \"type\", str(device)) == \"cuda\")\n",
    "\n",
    "# ========= 逆 ImageNet 归一化，用于更真实底图 =========\n",
    "IM_MEAN = np.array([0.485, 0.456, 0.406]).reshape(1,1,3)\n",
    "IM_STD  = np.array([0.229, 0.224, 0.225]).reshape(1,1,3)\n",
    "def de_norm_imagenet_to01(t):\n",
    "    if t.ndim == 4:\n",
    "        t = t[0]\n",
    "    arr = t.detach().float().cpu().permute(1,2,0).numpy()\n",
    "    arr = arr * IM_STD + IM_MEAN\n",
    "    return np.clip(arr, 0.0, 1.0)\n",
    "\n",
    "def _np01_to_u8rgb(arr01: np.ndarray) -> np.ndarray:\n",
    "    return np.clip(arr01 * 255.0, 0, 255).astype(np.uint8)\n",
    "\n",
    "def _label_img(pil_img: Image.Image, text: str, fill=(255,255,255)) -> Image.Image:\n",
    "    # 角标小字（无依赖字体，简单点）\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "    pad = 6\n",
    "    draw.rectangle([0, 0, draw.textlength(text)+2*pad, 20], fill=(0,0,0))\n",
    "    draw.text((pad, 3), text, fill=fill)\n",
    "    return pil_img\n",
    "\n",
    "def _make_2x2_grid(img_tl: Image.Image, img_tr: Image.Image,\n",
    "                   img_bl: Image.Image, img_br: Image.Image) -> Image.Image:\n",
    "    # 所有子图尺寸统一到 TL 尺寸\n",
    "    W, H = img_tl.size\n",
    "    def _fit(im): \n",
    "        return im.resize((W, H), Image.BILINEAR)\n",
    "    canvas = Image.new(\"RGB\", (W*2, H*2), (0,0,0))\n",
    "    canvas.paste(_fit(img_tl), (0,   0))\n",
    "    canvas.paste(_fit(img_tr), (W,   0))\n",
    "    canvas.paste(_fit(img_bl), (0,   H))\n",
    "    canvas.paste(_fit(img_br), (W,   H))\n",
    "    return canvas\n",
    "\n",
    "def run_gradcam_on_batch(sample):\n",
    "    sid = sample[\"id\"][0] if isinstance(sample[\"id\"], (list, tuple)) else sample[\"id\"]\n",
    "\n",
    "    x_r = sample.get(\"rnfl\", sample.get(\"image\")).to(device)\n",
    "    x_s = sample.get(\"slab\", None)\n",
    "    if x_s is not None:\n",
    "        x_s = x_s.to(device)\n",
    "\n",
    "    # 1) 索引边界检查（52 点）\n",
    "    if target_index is not None:\n",
    "        idx = int(target_index)\n",
    "        if not (0 <= idx < 52):\n",
    "            print(f\"[skip {sid}] target_index={idx} 越界(0..51)。\")\n",
    "            return sid\n",
    "\n",
    "    # 2) 目标（建议 sum：信号更强）\n",
    "    target = VFPointTarget(index=target_index)\n",
    "\n",
    "    # 3) RNFLT 分支 —— 另一分支显式 detach，保持数值前向一致\n",
    "    rnflt_base = to_numpy_rgb01(x_r)                         # [0,1] HWC\n",
    "    wrapper_r = DualWrapperRNFLT(model, fixed_slab=(x_s.detach() if x_s is not None else torch.zeros_like(x_r)))\n",
    "    cam_map_r = None\n",
    "    try:\n",
    "        with _make_cam(wrapper_r, [target_layer_r], reshape_fn, use_cuda) as cam_r:\n",
    "            with torch.enable_grad():\n",
    "                cam_map_r = cam_r(input_tensor=x_r, targets=[target])[0]\n",
    "    except Exception as e:\n",
    "        print(f\"[RNFLT CAM failed @ {sid}] {type(e).__name__}: {e}\")\n",
    "        try:\n",
    "            fallback = model.backbone_r.net.layer[-1].norm1\n",
    "            with _make_cam(wrapper_r, [fallback], reshape_fn, use_cuda) as cam_r2:\n",
    "                with torch.enable_grad():\n",
    "                    cam_map_r = cam_r2(input_tensor=x_r, targets=[target])[0]\n",
    "        except Exception as e2:\n",
    "            print(f\"[RNFLT fallback failed @ {sid}] {type(e2).__name__}: {e2}\")\n",
    "\n",
    "    # 4) SLAB 分支 —— 同理\n",
    "    slab_base = to_numpy_rgb01(x_s) if x_s is not None else None\n",
    "    cam_map_s = None\n",
    "    if x_s is not None:\n",
    "        wrapper_s = DualWrapperSLAB(model, fixed_rnflt=x_r.detach())\n",
    "        try:\n",
    "            with _make_cam(wrapper_s, [target_layer_s], reshape_fn, use_cuda) as cam_s:\n",
    "                with torch.enable_grad():\n",
    "                    cam_map_s = cam_s(input_tensor=x_s, targets=[target])[0]\n",
    "        except Exception as e:\n",
    "            print(f\"[SLAB CAM failed @ {sid}] {type(e).__name__}: {e}\")\n",
    "\n",
    "    # 5) 把四张图拼一张（若缺 slab，则用纯黑占位）\n",
    "    # RNFLT原图\n",
    "    rnflt_img = Image.fromarray(_np01_to_u8rgb(rnflt_base))\n",
    "    _label_img(rnflt_img, \"RNFL Image\")\n",
    "\n",
    "    # RNFLT热力叠加\n",
    "    if cam_map_r is not None:\n",
    "        vis_r = show_cam_on_image(rnflt_base, cam_map_r, use_rgb=True)  # uint8 RGB\n",
    "        rnflt_cam = Image.fromarray(vis_r)\n",
    "        _label_img(rnflt_cam, f\"RNFL CAM ({'idx '+str(target_index) if target_index is not None else 'sum'})\")\n",
    "    else:\n",
    "        rnflt_cam = Image.new(\"RGB\", rnflt_img.size, (0,0,0))\n",
    "        _label_img(rnflt_cam, \"RNFL CAM (N/A)\")\n",
    "\n",
    "    # SLAB原图\n",
    "    if slab_base is not None:\n",
    "        slab_img = Image.fromarray(_np01_to_u8rgb(slab_base))\n",
    "        _label_img(slab_img, \"SLAB Image\")\n",
    "    else:\n",
    "        slab_img = Image.new(\"RGB\", rnflt_img.size, (0,0,0))\n",
    "        _label_img(slab_img, \"SLAB Image (N/A)\")\n",
    "\n",
    "    # SLAB热力叠加\n",
    "    if cam_map_s is not None and slab_base is not None:\n",
    "        vis_s = show_cam_on_image(slab_base, cam_map_s, use_rgb=True)\n",
    "        slab_cam = Image.fromarray(vis_s)\n",
    "        _label_img(slab_cam, f\"SLAB CAM ({'idx '+str(target_index) if target_index is not None else 'sum'})\")\n",
    "    else:\n",
    "        slab_cam = Image.new(\"RGB\", rnflt_img.size, (0,0,0))\n",
    "        _label_img(slab_cam, \"SLAB CAM (N/A)\")\n",
    "\n",
    "    # 2x2 拼图：左上 RNFLT 原图，右上 RNFLT CAM，左下 SLAB 原图，右下 SLAB CAM\n",
    "    grid = _make_2x2_grid(rnflt_img, rnflt_cam, slab_img, slab_cam)\n",
    "    save_name = out_dir / f\"{sid}_combo_idx{target_index if target_index is not None else 'sum'}.png\"\n",
    "    grid.save(save_name)\n",
    "\n",
    "    # 同时保留你原来分开存的文件（如果不再需要可删除以下三段）\n",
    "    # Image.fromarray(show_cam_on_image(rnflt_base, cam_map_r, use_rgb=True)).save(out_dir / f\"{sid}_gradcam_rnflt_idx{target_index if target_index is not None else 'sum'}.png\")  # 可注释掉\n",
    "    # if cam_map_s is not None and slab_base is not None:\n",
    "    #     Image.fromarray(show_cam_on_image(slab_base, cam_map_s, use_rgb=True)).save(out_dir / f\"{sid}_gradcam_slab_idx{target_index if target_index is not None else 'sum'}.png\")\n",
    "\n",
    "    print(f\"[saved] {save_name}\")\n",
    "    \n",
    "    # =========================\n",
    "    # 额外保存：四联图（RNFLT, SLAB, GT, Prediction），Prediction 标题展示 MAE/RMAE\n",
    "    # 计算口径严格参考你之前的代码：用 numpy 对单样本做 mae/rmse\n",
    "    # =========================\n",
    "    try:\n",
    "        with torch.no_grad():\n",
    "            # 正常双路前向，得到 (1,52)\n",
    "            if x_s is not None:\n",
    "                out_scaled = model(rnflt=x_r, slab=x_s)           # 网络输出（训练刻度）\n",
    "            else:\n",
    "                out_scaled = model(rnflt=x_r, slab=torch.zeros_like(x_r))\n",
    "\n",
    "            out = (out_scaled * VFSCALE).detach().cpu().numpy()     # 还原到真实单位；形状 (1,52)\n",
    "            pred_52 = out.reshape(-1)                              # (52,)\n",
    "\n",
    "        # 拿 GT；没有就填 NaN\n",
    "        if \"label\" in sample and sample[\"label\"] is not None:\n",
    "            gt_52 = sample[\"label\"].detach().cpu().numpy().reshape(-1)\n",
    "        else:\n",
    "            gt_52 = np.full_like(pred_52, np.nan, dtype=np.float32)\n",
    "\n",
    "        # —— 指标：严格按你给的口径 —— #\n",
    "        if not np.isnan(gt_52).all():\n",
    "            diff  = pred_52 - gt_52\n",
    "            mae   = float(np.mean(np.abs(diff)))\n",
    "            rmse  = float(np.sqrt(np.mean(diff ** 2)))\n",
    "        else:\n",
    "            mae = rmse = float(\"nan\")\n",
    "\n",
    "        # Prediction 标题：显示 RMAE 和 MAE\n",
    "        pred_title = f\"Prediction  (RMSE={rmse:.3f}, MAE={mae:.3f})\" if np.isfinite(mae) else \"Prediction\"\n",
    "\n",
    "        # 使用你现有的四联图函数保存（函数内部会做反标准化/绘制 52 点）\n",
    "        quad_path = out_dir / f\"{sid}_quad_with_metrics.png\"\n",
    "        _plot_quad(\n",
    "            rnflt_3chw=x_r[0],                                         # (3,H,W) tensor\n",
    "            slab_3chw=(x_s[0] if x_s is not None else torch.zeros_like(x_r[0])),\n",
    "            gt_52=gt_52,                                               # numpy (52,)\n",
    "            pred_52=pred_52,                                           # numpy (52,)\n",
    "            titles=('RNFLT', 'SLAB', 'Ground truth', pred_title),\n",
    "            save_path=str(quad_path)\n",
    "        )\n",
    "        print(f\"[saved quad] {quad_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[quad save failed @ {sid}] {type(e).__name__}: {e}\")\n",
    "\n",
    "        \n",
    "    return sid\n",
    "\n",
    "\n",
    "# 迭代 DataLoader（这里就是 chosen_ids 的顺序）\n",
    "done = []\n",
    "for sample in val_loader:\n",
    "    sid = run_gradcam_on_batch(sample)\n",
    "    done.append(sid)\n",
    "\n",
    "print(\"Saved to:\", out_dir.resolve())\n",
    "print(\"Done IDs:\", done)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glaucoma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
